 
{ consider general class regularization methods learn vector parameters basis linear measurements
well known if regularizer nondecreasing function inner product then learned vector linear combination input data
result known representer theorem  at basis kernel-based methods machine learning
paper prove necessity above condition thereby completing characterization kernel methods based regularization
further extend our analysis regularization methods learn matrix problem motivated application multi-task learning
context study more general representer theorem holds larger class regularizers
provide necessary sufficient condition class matrix regularizers highlight them some concrete examples practical importance
our analysis uses basic principles matrix theory especially useful notion matrix nondecreasing function }
 introduction 
regularization hilbert spaces important methodology learning examples long history variety fields
been studied different perspectives statistics  optimal estimation recently been focus attention machine learning theory  see example references therein
regularization formulated optimization problem involving error term regularizer
regularizer plays important role favors solutions certain desirable properties
long been observed certain regularizers exhibit appealing property called representer theorem  states there exists solution regularization problem linear combination data
property important computational implications context regularization positive semidefinite kernels  because makes high infinite-dimensional problems type into finite dimensional problems size number available data
topic interest paper will determine conditions under representer theorems hold
first half paper describe property regularizer should satisfy order give rise representer theorem
turns out property simple geometric interpretation regularizer equivalently expressed nondecreasing function hilbert space norm
thus show condition already been known sufficient representer theorems also necessary
second half paper depart context hilbert spaces focus class problems matrix structure plays important role
problems recently appeared several machine learning applications show modified version representer theorem holds class regularizers significantly larger than former context
shall see matrix regularizers important context multi-task learning: matrix columns parameters different regression tasks regularizer encourages certain dependences across tasks
general consider problems framework tikhonov regularization
regularization approach receives set input/output data selects vector solution optimization problem
here prescribed hilbert space equipped inner product set possible output values
optimization problems encountered regularization type \min\left\{ \bigl \left w,x_1\rb,\dots,w,x_m \right  \left y_1 \dots y_m \right \bigr  \ \omega(w): w \right\} \ where regularization parameter
function called error function called regularizer
error function measures error data
typically decomposes sum univariate functions
example regression common choice would sum square errors
function  called regularizer favors certain regularity properties vector small norm chosen based available prior information about target vector
some hilbert spaces sobolev spaces regularizer measure smoothness: smaller norm smoother function
framework includes several well-studied learning algorithms ridge regression  support vector machines  many more  see references therein
important aspect practical success approach observation certain choices regularizer solving \eqref{eq:reg_intro} reduces identifying parameters not
specifically when regularizer square hilbert space norm representer theorem holds: there exists solution \eqref{eq:reg_intro} linear combination input vectors {w}  \sum_{i=1}^m c_i x_i where some real coefficients
result simple prove dates at least 1970's see example
also known extends any regularizer nondecreasing function norm
several other variants results about representation form \eqref{eq:rt} also appeared recent years
moreover representer theorem been important machine learning particularly within context learning reproducing kernel hilbert spaces  see references therein
our first objective paper derive necessary sufficient conditions representer theorems hold
even though one mainly interested regularization problems more convenient study interpolation problems problems form \min\left\{ \omega(w): w  w,x_i y_i,~i=1,\dots,m \right\} \
thus begin paper section  showing how representer theorems interpolation regularization relate
one side representer theorem interpolation easily implies theorem regularization same regularizer any error function
therefore all representer theorems obtained paper apply equally interpolation regularization
other side though converse implication true under certain weak qualifications error function
having addressed issue concentrate section proving interpolation problem \eqref{eq:int_intro} admits solutions representable form \eqref{eq:rt} if only if regularizer nondecreasing function hilbert space norm
provide complete characterization regularizers give rise representer theorems had been open question
furthermore discuss how our proof motivated geometric understanding representer theorem equivalently expressed monotonicity property regularizer
our second objective formulate study novel question representer theorems matrix problems
make our discussion concrete let us consider problem learning linear regression vectors represented parameters  respectively
each vector thought ``task'' goal jointly learn tasks
problems there usually prior knowledge relates tasks often case learning improve if knowledge appropriately taken into account
consequently good regularizer should favor task relations involve all tasks jointly
case interpolation learning framework formulated concisely } where denotes set real matrices column vectors form matrix
each task its own input data corresponding output values
important feature problems distinguishes them type \eqref{eq:int_intro} appearance matrix products constraints unlike inner products \eqref{eq:int_intro}
fact will discuss section  problems type \eqref{eq:matrix_intro} written form \eqref{eq:int_intro}
consequently representer theorem applies if matrix regularizer nondecreasing function frobenius norm
however optimal vector each task represented linear combination only those input vectors corresponding particular task
moreover regularizers easy see each task \eqref{eq:matrix_intro} optimized independently
hence regularizers no practical interest if tasks expected related
observation leads us formulate modified representer theorem  appropriate matrix problems namely where scalar coefficients
other words now allow all input vectors present linear combination representing each column optimal matrix
result definition greatly expands class regularizers give rise representer theorems
moreover framework applied many applications where matrix optimization problems involved
our immediate motivation however been more specific than namely multi-task learning
learning multiple tasks jointly been growing area interest machine learning especially during past few years
instance some works use regularizers involve trace norm matrix
general idea behind methodology small trace norm favors low-rank matrices
means tasks columns  related they all lie low-dimensional subspace
case trace norm representer theorem \eqref{eq:rep_matrix_intro} known hold  see  also discussed section
natural therefore ask question similar standard hilbert space single-task setting
under conditions regularizer representer theorem holds
section  provide answer proving necessary sufficient condition representer theorems hold expressed simple monotonicity property
property analogous one hilbert space setting but its geometric interpretation now algebraic nature
also give functional description equivalent property show regularizers interest matrix nondecreasing functions quantity
our results cover matrix problems type \eqref{eq:matrix_intro} already been studied literature
but they also point towards some new learning methods may perform well practice now made computationally efficient
thus close paper discussion possible regularizers satisfy our conditions been used used future machine learning problems
