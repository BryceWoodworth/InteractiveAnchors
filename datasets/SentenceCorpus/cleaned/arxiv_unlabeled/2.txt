 
problem joint universal source coding modeling addressed rissanen context lossless codes generalized fixed-rate lossy coding continuous-alphabet memoryless sources
show bounded distortion measures any compactly parametrized family iid
real vector sources absolutely continuous marginals satisfying appropriate smoothness vapnik--chervonenkis learnability conditions admits joint scheme universal lossy block coding parameter estimation give nonasymptotic estimates convergence rates distortion redundancies variational distances between active source estimated source
also present explicit examples parametric sources admitting joint universal compression modeling schemes
 introduction 
universal data compression single code achieves asymptotically optimal performance all sources within given family
intuition suggests good universal coder should acquire accurate model source statistics sufficiently long data sequence incorporate knowledge its operation
lossless codes intuition been made rigorous rissanen
under his scheme data encoded two-stage set-up binary representation each source block consists two parts: 1 suitably quantized maximum-likelihood estimate source parameters 2 lossless encoding data matched acquired model; redundancy resulting code converges zero  where block length
paper extend rissanen's idea lossy block coding vector quantization iid
sources values some finite
specifically let iid
source marginal distribution belonging some indexed class absolutely continuous distributions  where bounded subset some
bounded distortion measures our main result theorem states if class satisfies certain smoothness learnability conditions then there exists sequence finite-memory lossy block codes achieves asymptotically optimal compression each source class permits asymptotically exact identification active source respect variational distance  defined  where supremum over all borel subsets
overhead rate distortion redundancy scheme converge zero  respectively where block length while active source identified up variational ball radius eventually almost surely
also describe extension our scheme unbounded distortion measures satisfying certain moment condition present two examples parametric families satisfying regularity conditions theorem
while most existing schemes universal lossy coding rely implicit identification active source e g  through topological covering arguments  glivenko--cantelli uniform laws large numbers  nearest-neighbor code clustering  our code builds explicit model mechanism responsible generating data then selects appropriate code data basis model
ability simultaneously model compress data may prove useful applications media forensics  where parameter could represent evidence tampering aim compress data way evidence later extracted high fidelity compressed version
another key feature our approach use vapnik--chervonenkis theory order connect universal encodability class sources combinatorial ``richness" certain collection decision regions associated sources
way vapnik--chervonenkis estimates thought imperfect analogue combinatorial method types finite alphabets
