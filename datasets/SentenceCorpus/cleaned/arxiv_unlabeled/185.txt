 
problem statistical learning construct accurate predictor random variable function correlated random variable basis iid \ training sample their joint distribution
allowable predictors constrained lie some specified class goal approach asymptotically performance best predictor class
consider two settings learning agent only access rate-limited descriptions training data present information-theoretic bounds predictor performance achievable presence communication constraints
our proofs do not assume any separation structure between compression learning rely new class operational criteria specifically tailored joint design encoders learning algorithms rate-constrained settings
 introduction 
let jointly distributed random variables
problem statistical learning design accurate predictor output variable input variable basis number independent training samples drawn their joint distribution very little no prior knowledge distribution
present paper focuses achievable performance learning schemes when learning agent only access finite-rate description training samples
problem learning under communication constraints arises variety contexts distributed estimation using sensor network adaptive control repeated games
other scenarios often case agents who gather training data geographically separated agents who use data make inferences decisions communication between two types agents possible only over rate-limited channels
hence there trade-off between communication rate quality inference interest characterize trade-off mathematically
paper follows our earlier work presents improved bounds achievable performance statistical learning schemes operating under two kinds communication constraints: entire training sequence delivered learning agent over rate-limited noiseless digital channel b input part training sequence available learning agent arbitrary precision while output part delivered before over rate-limited channel
whereas looked at schemes where finite-rate description training data was obtained through vector quantization effectively imposing separation structure between compression learning here remove restriction
show under certain regularity conditions there no penalty compression training sequence setting
due fact encoder reliably estimate underlying distribution metric specifically tailored learning problem at hand then communicate finite-rate description learning agent who then find optimum predictor estimated distribution
setting b however radically different: because encoder no access input part training sample cannot estimate underlying distribution
instead encoder constructs finite-rate description output part using specific kind vector quantizer namely one designed minimize expected distance between underlying distribution whatever may happen empirical distribution input/quantized output pairs
our achievability result setting b uses learning-theoretic generalization recent work kramer savari rate-constrained communication probability distributions
problem learning pattern classifier under rate constraints was also treated recent paper westover o'sullivan
they assumed underlying probability distribution known rate constraint arises limitations memory learning agent; then problem design best possible classifier without any constraints its structure
motivation work comes biologically inspired models learning
approach present paper complementary
consider more general decision-theoretic formulation learning includes regression well classification but allow only vague prior knowledge underlying distribution assume class available predictors constrained
thus while presents information-theoretic bounds performance any classifier including ones fully cognizant generative model data here concerned performance constrained learning schemes must perform well presence uncertainty about underlying distribution
novel element our approach both operational criteria used design encoders learning algorithm regularity conditions must hold rate-constrained learning possible involve tight coupling between available prior knowledge about underlying distribution set predictors available learning agent
planned future work includes obtaining converse theorems lower bounds applying our formalism specific classes predictors used statistical learning theory
