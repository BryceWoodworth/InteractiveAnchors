### abstract ###
This article treats the problem of learning a dictionary providing sparse representations for a given signal class, via  SYMBOL -minimisation
The problem can also be seen as factorising a  SYMBOL  matrix  SYMBOL  of training signals into a  SYMBOL  dictionary matrix  SYMBOL  and a  SYMBOL  coefficient matrix   SYMBOL , which is sparse
The exact question studied here is when a dictionary coefficient pair  SYMBOL  can be recovered as local minimum of a (nonconvex)  SYMBOL -criterion with input  SYMBOL
First, for general dictionaries and coefficient matrices, algebraic conditions ensuring local identifiability  are derived, which are then specialised to the case when the dictionary is a basis
Finally, assuming a random Bernoulli-Gaussian sparse model on the coefficient matrix, it is shown that sufficiently incoherent bases are locally identifiable with high probability
The perhaps surprising result is that the typically sufficient number of training samples  SYMBOL  grows up to a logarithmic factor only linearly with the signal dimension, ie SYMBOL , in contrast to previous approaches requiring combinatorially many samples
### introduction ###
Many signal processing tasks, such as denoising and compression, can be efficiently performed if one knows a sparse representation of the signals of interest
Moreover, a huge body of recent results on sparse representations has highlighted their impact on inverse linear problems such as (blind) source separation and localisation as well as compressed sampling, for a starting point see eg CITATION \\ In any of these publications, one will - more likely than not - find a statement starting with 'given a dictionary  SYMBOL  and a signal  SYMBOL  having an  SYMBOL -sparse approximation/representation  SYMBOL  \ldots', which points exactly to the remaining problem: all applications of sparse representations rely on a signal dictionary  SYMBOL  from which sparse linear expansions can be built that efficiently approximate the signals from a class of interest; success heavily depends on the good fit between the data class and the dictionary \\ For many signal classes, good dictionaries -- such as time-frequency or time-scale dictionaries -- are known, but new data classes may require the construction of new dictionaries to fit new types of data features
The analytic construction of dictionaries such as wavelets and curvelets stems from deep  mathematical tools from Harmonic Analysis
It may, however, be difficult and time consuming to develop complex mathematical theory each time a new class of data, which requires a different type of dictionary,  is met
An alternative approach is dictionary learning, which aims at infering the dictionary  SYMBOL  from a set of training data  SYMBOL
Dictionary learning, also known as  sparse coding , has the potential of 'industrialising' sparse representation techniques for new data classes \\ This article treats the theoretical dictionary learning problem, expressed as a factorisation problem which consists of identifying a  SYMBOL  matrix  SYMBOL  from a set of  SYMBOL  observed training vectors  SYMBOL ,  knowing that  SYMBOL ,  SYMBOL  for some unknown collection of coefficient vectors  SYMBOL  with certain statistical properties \\ Considering the extensive literature available for the sparse decomposition problem after the early work in ~ CITATION , surprisingly little work has been dedicated to theoretical dictionary learning so far
There exist several dictionary learning algorithms (see eg CITATION ), but only recently people have started to consider also the theoretical aspects of the problem
The origins of research into what is now called dictionary learning can be found in the field of Independent Component Analysis (ICA)~ CITATION
There, many identifiability results are available, which, however, rely on  asymptotic  statistical properties under  statistical independence  and  non-Gaussianity  assumptions \\ In contrast, Georgiev, Theis and Cichocki,  CITATION , as well as Aharon, Elad and Bruckstein,  CITATION , described more geometric identifiability conditions on the sparse coefficients of training data in an ideal (overcomplete) dictionary
Yet, for these conditions to hold, the size  SYMBOL  of the training set seems to be required to grow exponentially fast with the number of atoms  SYMBOL , and the provably good identification algorithms are combinatorial
Moreover, the algorithms and the identifiability analysis are not robust to 'outliers', i e , training samples  SYMBOL  where  SYMBOL  fails to be sufficiently sparse
For applications, on the other hand, we are concerned with relatively large-dimensional data (e g SYMBOL , or even  SYMBOL ) but limited availability of training data ( SYMBOL  is not much larger than say  SYMBOL ) as well as limited computational resources \\ In this article, we study the possibility of designing provably good, non-combinatorial dictionary learning algorithms that are robust to outliers and to the limited availability of training samples
Inspired by recent proofs of good properties of  SYMBOL -minimisation for sparse signal decomposition with a given dictionary, we investigate the properties of  SYMBOL -based dictionary learning,  CITATION
Our ultimate goal, described in details in Section~, is to characterise properties that a set of training samples  SYMBOL  should satisfy to guarantee that an ideal dictionary is the only local minimum of the  SYMBOL -criterion, opening up the possibility of replacing combinatorial learning algorithms with efficient numerical descent techniques
As a first step, we investigate conditions under which an ideal dictionary is a local minimum of the  SYMBOL -criterion \\ {Main results }  First,  we describe the proposed setting in Section~ and characterise the local minima of the  SYMBOL -cost function in Section~
We discuss the geometrical interpretation of this characterisation in Section~
Then, using concentration of measure, we prove in Section~ the perhaps surprising result that when    SYMBOL  if the samples  SYMBOL , are a typical draw from a Bernoulli-Gaussian random distribution (which can generate a large proportion of  outliers ), then any sufficiently incoherent basis matrix  SYMBOL ,  SYMBOL , is a local minimum of the cost function and is therefore 'locally identifiable'
The constant  SYMBOL  depends on a parameter of the Bernoulli-Gaussian distribution which drives the sparsity of the training set \\ This number of training samples is surprisingly small considering that  SYMBOL  training samples provide  SYMBOL  real parameters, while the basis matrix  SYMBOL  is essentially parameterised by  SYMBOL  independent real parameters \\ In the considered matrix identification setting, it should be noted that  SYMBOL  is  not  a convex cost function
It admits  several local minima  hence local identifiability only implies that, upon good initial conditions, numerical optimisation schemes performing the  SYMBOL -optimisation will recover the desired matrix  SYMBOL
However, empirical experiments in low dimension ( SYMBOL ), shown in Section~, indicate that for typical draws of Bernoulli-Gaussian training samples  SYMBOL , the matrix  SYMBOL  is in fact the  only  local minimum of the criterion (up to natural indeterminacies of the problem such as column permutation)
If this empirical observation could be turned into a theorem for general dimension  SYMBOL  under the Bernoulli-Gaussian sparse model, this would imply that typically: a)  SYMBOL -minimisation is a good  identification principle ; b) any decent  SYMBOL -descent algorithm is a good  identification algorithm
makros
